# GNU Test Artifacts Analysis Tools

This directory contains tools to download GitHub Actions artifacts from the GnuTests.yml workflow and generate comprehensive statistics and visualizations from the test results.

## Scripts

### 1. `download.py`

Downloads artifacts from GitHub Actions workflow runs and generates detailed statistics.

**Features:**
- Downloads `aggregated-result` artifacts from recent workflow runs
- Analyzes test results across multiple runs
- Generates comprehensive statistics including:
  - Overall success/failure rates
  - Per-utility statistics
  - Trend analysis over time
  - Identification of flaky tests
  - Most frequently failing tests
  - Test status history

**Usage:**
```bash
# Basic usage (requires GITHUB_TOKEN environment variable)
python download.py

# With custom parameters
python download.py \
    --token YOUR_GITHUB_TOKEN \
    --repo uutils/coreutils \
    --limit 20 \
    --output-dir ./my-artifacts \
    --stats-file ./my-statistics.json \
    --verbose

# Using environment variable for token
export GITHUB_TOKEN=your_token_here
python download.py --limit 50 --verbose
```

**Requirements:**
```bash
pip install requests
```

### 2. `visualize.py`

Creates reports and visualizations from the statistics generated by the downloader script.

**Features:**
- Generates Markdown, HTML, or JSON reports
- Creates charts and graphs (optional, requires matplotlib)
- Shows trend analysis, utility comparisons, and failure patterns
- Identifies problematic and flaky tests

**Usage:**
```bash
# Generate a Markdown report
python visualize.py gnu-test-statistics.json

# Generate HTML report with charts
python visualize.py gnu-test-statistics.json \
    --format html \
    --charts \
    --output-dir ./reports

# Generate charts (requires matplotlib and seaborn)
pip install matplotlib seaborn
python visualize.py gnu-test-statistics.json --charts
```

## Quick Start

1. **Set up your GitHub token:**
   ```bash
   export GITHUB_TOKEN=your_personal_access_token
   ```

2. **Download artifacts and generate statistics:**
   ```bash
   python util/gnu-test-analysis/download.py --limit 25 --verbose
   ```

3. **Generate a report with visualizations:**
   ```bash
   pip install matplotlib seaborn  # For charts
   python util/gnu-test-analysis/visualize.py gnu-test-statistics.json --charts
   ```

4. **View the results:**
   - Statistics: `gnu-test-statistics.json`
   - Report: `reports/gnu-test-report.md`
   - Charts: `reports/*.png`

## GitHub Token Setup

You need a GitHub personal access token to access the GitHub API:

1. Go to https://github.com/settings/tokens
2. Click "Generate new token (classic)"
3. Select scopes: `repo` (for private repos) or `public_repo` (for public repos)
4. Copy the token and set it as an environment variable:
   ```bash
   export GITHUB_TOKEN=your_token_here
   ```

## Output Files

### Statistics JSON Structure

The generated statistics file contains:

```json
{
  "metadata": {
    "generated_at": "2025-01-18T10:30:00",
    "runs_analyzed": 25,
    "total_test_executions": 50000
  },
  "overall_statistics": {
    "total_tests": 50000,
    "pass": 45000,
    "fail": 3000,
    "skip": 1800,
    "error": 200,
    "overall_success_rate": 90.0,
    "failure_rate": 6.4
  },
  "per_utility_statistics": {
    "cp": {
      "total_tests": 1500,
      "pass": 1400,
      "fail": 80,
      "skip": 20,
      "error": 0,
      "overall_success_rate": 93.33,
      "avg_success_rate": 92.5,
      "min_success_rate": 88.0,
      "max_success_rate": 95.0
    }
  },
  "most_failing_tests": [
    {
      "test": "cp::cp-special-files",
      "failure_count": 12
    }
  ],
  "flaky_tests": [
    {
      "test": "sort::sort-numeric-unstable",
      "flaky_score": 0.8,
      "results": ["PASS", "FAIL", "PASS", "FAIL", "PASS"],
      "total_runs": 25
    }
  ],
  "trend_analysis": [
    {
      "run_number": 20800,
      "date": "2025-01-18T08:00:00Z",
      "success_rate": 89.5,
      "total_tests": 2000
    }
  ]
}
```

### Generated Reports

- **Markdown Report** (`reports/gnu-test-report.md`): Human-readable summary with tables
- **HTML Report** (`reports/gnu-test-report.html`): Web-viewable version of the report
- **Charts** (`reports/*.png`): Visual representations of trends and statistics

## Examples

### Download last 50 runs and generate trend analysis

```bash
# Download last 50 runs and generate trend analysis
python util/gnu-test-analysis/download.py --limit 50
python util/gnu-test-analysis/visualize.py gnu-test-statistics.json --charts
```

### Focus on Recent Activity

```bash
# Download only last 10 runs for quick analysis
python util/gnu-test-analysis/download.py --limit 10
python util/gnu-test-analysis/visualize.py gnu-test-statistics.json --format html
```

### Analyze Different Repository

```bash
# Analyze a fork or different repository
python util/gnu-test-analysis/download.py \
    --repo yourusername/coreutils \
    --limit 15
```

## Troubleshooting

### Common Issues

1. **GitHub API Rate Limits:**
   - The script respects rate limits but may be slow with many requests
   - Use `--limit` to reduce the number of runs processed

2. **Missing Artifacts:**
   - Some workflow runs may not have `aggregated-result` artifacts
   - The script will skip these runs and continue

3. **Large Output Files:**
   - Statistics files can be large with many runs
   - Consider using `--limit` to control size

4. **Charts Not Generated:**
   - Install matplotlib and seaborn: `pip install matplotlib seaborn`
   - Use `--charts` flag with the visualizer

### Debug Mode

Add `--verbose` to either script for detailed output:

```bash
python util/gnu-test-analysis/download.py --verbose --limit 5
python util/gnu-test-analysis/visualize.py gnu-test-statistics.json --verbose --charts
```

## Integration with CI/CD

You can integrate these scripts into your own workflows:

```yaml
- name: Download and analyze GNU test artifacts
  run: |
    export GITHUB_TOKEN=${{ secrets.GITHUB_TOKEN }}
    python util/gnu-test-analysis/download.py --limit 20
    python util/gnu-test-analysis/visualize.py gnu-test-statistics.json --format html

- name: Upload analysis results
  uses: actions/upload-artifact@v4
  with:
    name: gnu-test-analysis
    path: |
      gnu-test-statistics.json
      reports/
```

This allows you to regularly monitor the health of your GNU test suite and identify patterns in test failures.
